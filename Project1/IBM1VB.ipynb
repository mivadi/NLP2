{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2 project 1\n",
    "### Mirthe van Diepen and Fabio Curi\n",
    "\n",
    "First import required libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from collections import *\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from decimal import Decimal\n",
    "from matplotlib.ticker import NullFormatter  # useful for `logit` scale\n",
    "\n",
    "from scipy import special\n",
    "\n",
    "from scipy import *\n",
    "import scipy \n",
    "import os\n",
    "\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "np.random.seed(19680801)\n",
    "\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(tar, names, en_words_u = [], fr_words_u = [], en_words_k = [], fr_words_k = []):\n",
    "    \n",
    "    for member in tar.getmembers():\n",
    "        # check if we want to read this file\n",
    "        if not member.name in names: continue\n",
    "        if member.name == names[0]:\n",
    "            en_tr = tar.extractfile(member).readlines()\n",
    "        elif member.name == names[1]:\n",
    "            fr_tr = tar.extractfile(member).readlines()\n",
    "\n",
    "    # Create dataframe\n",
    "    data = pd.DataFrame({'en': en_tr,'fr': fr_tr})\n",
    "\n",
    "    # Get strings and delete \\n \n",
    "    for i in data.index:\n",
    "        data.at[i, 'en'] = data.at[i, 'en'].decode(\"utf-8\")\n",
    "        data.at[i, 'fr'] = data.at[i, 'fr'].decode(\"utf-8\")\n",
    "        if ' \\n' in data.at[i, 'en']:\n",
    "            data.at[i, 'en'] = data.at[i, 'en'].replace(' \\n','')\n",
    "        if ' \\n' in data.at[i, 'fr']:\n",
    "            data.at[i, 'fr'] = data.at[i, 'fr'].replace(' \\n','')\n",
    "\n",
    "    # create count dictionary \n",
    "    dict_e = defaultdict(lambda: 0)\n",
    "    dict_f = defaultdict(lambda: 0)\n",
    "    for i in data.index:\n",
    "        tokens_e = data.at[i, 'en'].split(' ')\n",
    "        for token in tokens_e:\n",
    "            dict_e[token] += 1\n",
    "        tokens_f = data.at[i, 'fr'].split(' ')\n",
    "        for token in tokens_f:\n",
    "            dict_f[token] += 1\n",
    "                \n",
    "    # Get all bilingual sentences\n",
    "    sentences_u = []\n",
    "    sentences_k = []\n",
    "    for x,y in zip(data['en'], data['fr']):\n",
    "        sentences_u.append([x.split(),y.split()])\n",
    "        sentences_k.append([x.split(),y.split()])\n",
    "\n",
    "    if len(en_words_u) + len(fr_words_u) + len(en_words_k) + len(fr_words_k) == 0:\n",
    "        for s in sentences_u:\n",
    "            for i in range(len(s[0])):\n",
    "                if dict_e[s[0][i]] < 5:\n",
    "                    # replace rare words by 'unk'\n",
    "                    s[0][i] = 'unk'\n",
    "                en_words_u.append(s[0][i])\n",
    "            for i in range(len(s[1])):\n",
    "                if dict_f[s[1][i]] < 5:\n",
    "                    # replace rare words by 'unk'\n",
    "                    s[1][i] = 'unk'\n",
    "                fr_words_u.append(s[1][i])\n",
    "            # add null\n",
    "            s[0] = ['NULL'] + s[0]\n",
    "        for s in sentences_k:\n",
    "            for i in range(len(s[0])):\n",
    "                en_words_k.append(s[0][i])\n",
    "            for i in range(len(s[1])):\n",
    "                fr_words_k.append(s[1][i])\n",
    "            # add null\n",
    "            s[0] = ['NULL'] + s[0]\n",
    "        en_words_u, fr_words_u = sorted(list(set(en_words_u))) , sorted(list(set(fr_words_u)))\n",
    "        en_words_k, fr_words_k = sorted(list(set(en_words_k))) , sorted(list(set(fr_words_k)))\n",
    "        print('Size of English dictionary with unknown words:', len(en_words_u))\n",
    "        print('Size of French dictionary with unknown words:', len(fr_words_u))\n",
    "        print('Size of English dictionary for k-smoothing:', len(en_words_k))\n",
    "        print('Size of French dictionary for k-smoothing:', len(fr_words_k))\n",
    "        return sentences_u, sentences_k, en_words_u, fr_words_u,  en_words_k, fr_words_k\n",
    "    else:\n",
    "        for s in sentences_u:\n",
    "            for i in range(len(s[0])):\n",
    "                if not s[0][i] in en_words_u:\n",
    "                    # replace rare words by 'unk'\n",
    "                    s[0][i] = 'unk'\n",
    "            for i in range(len(s[1])):\n",
    "                if not s[1][i] in fr_words_u:\n",
    "                    # replace rare words by 'unk'\n",
    "                    s[1][i] = 'unk'\n",
    "            # add null\n",
    "            s[0] = ['NULL'] + s[0]\n",
    "            \n",
    "        for s in sentences_k:\n",
    "            # add null\n",
    "            s[0] = ['NULL'] + s[0]\n",
    "            \n",
    "        return data, sentences_u, sentences_k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English dictionary with unknown words: 15867\n",
      "Size of French dictionary with unknown words: 19073\n",
      "Size of English dictionary for k-smoothing: 36635\n",
      "Size of French dictionary for k-smoothing: 46421\n"
     ]
    }
   ],
   "source": [
    "# Read training data\n",
    "tar = tarfile.open(\"training.tgz\", \"r:gz\")\n",
    "names = [\"training/hansards.36.2.e\", \"training/hansards.36.2.f\"]\n",
    "sentences_u, sentences_k, en_words_u, fr_words_u,  en_words_k, fr_words_k = read_data(tar, names)\n",
    "\n",
    "# Read test data\n",
    "tar = tarfile.open(\"testing.tgz\", \"r:gz\")\n",
    "names = [\"testing/test/test.e\", \"testing/test/test.f\"]\n",
    "test_data, test_sentences_u, test_sentences_k = read_data(tar, names, en_words_u, fr_words_u,  en_words_k, fr_words_k)\n",
    "\n",
    "# Read validation data\n",
    "tar = tarfile.open(\"validation.tgz\", \"r:gz\")\n",
    "names = [\"validation/dev.e\", \"validation/dev.f\", \"dev.wa.nonullalign\"]\n",
    "_, validation_sentences_u, validation_sentences_k = read_data(tar, names, en_words_u, fr_words_u,  en_words_k, fr_words_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evolution of alignment error rate (AER) on validation data as a function of the iteration.\n",
    "\n",
    "\"\"\"\n",
    "This module helps you compute AER for validation set after each iteration of EM.\n",
    "Check the test function below for an example of how to use these helper functions.\n",
    "\n",
    "For test results, please use the official AER perl script.\n",
    "\"\"\"\n",
    "\n",
    "def read_naacl_alignments(path):\n",
    "    \"\"\"\n",
    "    Read NAACL-formatted alignment files.\n",
    "\n",
    "    :param path: path to file\n",
    "    :return: a list of pairs [sure set, possible set]\n",
    "        each entry in the set maps an input position to an output position\n",
    "        sentences start from 1 and a NULL token is indicated with position 0\n",
    "    \"\"\"\n",
    "    with open(path) as fi:\n",
    "        ainfo = {}\n",
    "        for i, line in enumerate(fi.readlines()):\n",
    "            fields = line.split()\n",
    "            if not fields:\n",
    "                continue\n",
    "            sure = True  # by default we assumed Sure links\n",
    "            prob = 1.0  # by default we assume prob 1.0\n",
    "            if len(fields) < 3:\n",
    "                raise ValueError('Missing required fields in line %d: %s' % (i, line.strip()))\n",
    "            snt_id, x, y = int(fields[0]), int(fields[1]), int(fields[2])\n",
    "            if len(fields) == 5:\n",
    "                sure = fields[3] == 'S'\n",
    "                prob = float(fields[4])\n",
    "            if len(fields) == 4:\n",
    "                if fields[3] in {'S', 'P'}:\n",
    "                    sure = fields[3] == 'S'\n",
    "                else:\n",
    "                    prob = float(fields[3])\n",
    "            snt_info = ainfo.get(snt_id, None)\n",
    "            if snt_info is None:\n",
    "                snt_info = [set(), set()]  # S and P sets\n",
    "                ainfo[snt_id] = snt_info\n",
    "            if sure:  # Note that S links are also P links: http://dl.acm.org/citation.cfm?id=992810\n",
    "                snt_info[0].add((x, y))\n",
    "                snt_info[1].add((x, y))\n",
    "            else:\n",
    "                snt_info[1].add((x, y))\n",
    "    return tuple(v for k, v in sorted(ainfo.items(), key=lambda pair: pair[0]))\n",
    "\n",
    "\n",
    "class AERSufficientStatistics:\n",
    "    \"\"\"\n",
    "    Object used to compute AER for a corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.a_and_s = 0\n",
    "        self.a_and_p = 0\n",
    "        self.a = 0\n",
    "        self.s = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s/%s/%s/%s %s' % (self.a_and_s, self.a_and_p, self.a, self.s, self.aer())\n",
    "\n",
    "    def update(self, sure, probable, predicted):\n",
    "        \"\"\"\n",
    "        Update AER sufficient statistics for a set of predicted links given goldstandard information.\n",
    "\n",
    "        :param sure: set of sure links \n",
    "            a links is a tuple of 1-based positions (from, to) where 0 is reserved for NULL\n",
    "        :param probable: set of probable links (must incude sure links)\n",
    "        :param predicted: set of predicted links\n",
    "        \"\"\"\n",
    "        self.a_and_s += len(predicted & sure)\n",
    "        self.a_and_p += len(predicted & probable)\n",
    "        self.a += len(predicted)\n",
    "        self.s += len(sure)\n",
    "\n",
    "    def aer(self):\n",
    "        \"\"\"Return alignment error rate: 1 - (|A & S| + |A & P|)/(|A| + |S|)\"\"\"\n",
    "        return 1 - (self.a_and_s + self.a_and_p) / (self.a + self.s)\n",
    "\n",
    "\n",
    "def test(path, predictions):\n",
    "    \n",
    "    # read in gold alignments\n",
    "    gold_sets = read_naacl_alignments(path)\n",
    "\n",
    "    # compute AER:\n",
    "    print(type(gold_sets))\n",
    "    print(type(predictions))\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    \n",
    "    return metric.aer()\n",
    "\n",
    "def all_indices(value, qlist):\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    while True:\n",
    "        try:\n",
    "            idx = qlist.index(value, idx+1)\n",
    "            indices.append(idx)\n",
    "        except ValueError:\n",
    "            break\n",
    "    return indices[0]\n",
    "\n",
    "def align_IBM1(s, t):\n",
    "    \n",
    "    alignments=[]\n",
    "    l = len(s[0])\n",
    "    m = len(s[1])\n",
    "    for i in range(l):\n",
    "        if i==0: continue\n",
    "        vals=[]\n",
    "        for j in range(m):\n",
    "            vals.append(t[s[0][i]][s[1][j]])\n",
    "        max_idx = vals.index(max(vals))\n",
    "        alignments.append((i, max_idx+1)) # try +1\n",
    "\n",
    "    return set(alignments)\n",
    "\n",
    "def align_IBM2(s, pi_t, pi_A):\n",
    "    \n",
    "    alignments=[]\n",
    "    l = len(s[0])\n",
    "    m = len(s[1])\n",
    "    for i in range(l):\n",
    "        if i==0: continue\n",
    "        vals=[]\n",
    "        for j in range(m):\n",
    "            vals.append(pi_A[l][m][j][i] * pi_t[s[0][i]][s[1][j]])\n",
    "        max_idx = vals.index(max(vals))\n",
    "        alignments.append((i, max_idx+1))\n",
    "\n",
    "    return set(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference for Bayesian IBM model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ELBO(psi, alpha, lamb, theta, epsilon, sentences):\n",
    "    \n",
    "    Vf = len(fr_words_u)\n",
    "    Ve = len(en_words_u)\n",
    "    part1 = 0\n",
    "    part2 = 0\n",
    "    \n",
    "    lamb_fr = defaultdict(lambda: 0.)\n",
    "    for k, s in enumerate(sentences):\n",
    "        e, f = s[0], s[1]\n",
    "        l = len(s[0])\n",
    "        m = len(s[1])\n",
    "        for j in range(m):\n",
    "            Q = epsilon\n",
    "            for i in range(l):\n",
    "                e, f = s[0][i], s[1][j]\n",
    "                Q *= psi[k][e][f]**i\n",
    "                part2 += theta[e][f]*(alpha-lamb[e][f]) + log(scipy.stats.gamma(lamb[e][f]))\n",
    "                lamb_fr[e] += lamb[e][f]\n",
    "            part1 += log(Q)\n",
    "    for e in en_words_u:\n",
    "        part2 -= log(scipy.stats.gamma(lamb_fr[e]))\n",
    "    part2 += Ve * log(scipy.stats.gamma(Vf * alpha))\n",
    "    \n",
    "    return part1 + part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha is not depending on f\n",
    "\n",
    "als = []\n",
    "\n",
    "def VIB(sentences, val_sentences, alpha=0.1, iterations=10):\n",
    "    \n",
    "    lambd = defaultdict(lambda: defaultdict(lambda: 0.5))\n",
    "    theta = defaultdict(lambda: defaultdict(lambda: 0.5))\n",
    "    \n",
    "    # initialise counts\n",
    "    lambda_total = defaultdict(lambda: 0.)\n",
    "    theta_total = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "    count_e2f = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "    \n",
    "    perplexity_vals, AER_vals, house_maison, house_demain = [], [], [], []\n",
    "    \n",
    "    start = time.time()\n",
    "    for s in sentences:\n",
    "        for e in s[0]:\n",
    "            for f in s[1]:\n",
    "                lambda_total[e] += lambd[e][f]\n",
    "                count_e2f[e][f] += 1\n",
    "    end = time.time()\n",
    "    print('first loop: {}'.format(end-start))\n",
    "    \n",
    "    for p in range(iterations):\n",
    "        \n",
    "        print(\"Iteration {}\".format(p))     \n",
    "        \n",
    "        # E-step\n",
    "        start = time.time()\n",
    "        for k, s in enumerate(sentences):\n",
    "            for e in s[0]:\n",
    "                theta_total[e] = 0\n",
    "                for f in s[1]:\n",
    "                    theta[e][f] = np.exp(scipy.special.digamma(lambd[e][f])- scipy.special.digamma(lambda_total[e]))\n",
    "                    theta_total[k][f] += theta[e][f]\n",
    "        end = time.time()\n",
    "        print('second loop: {}'.format(end-start))\n",
    "        \n",
    "        # M-step\n",
    "        start = time.time()\n",
    "        for k, s in enumerate(sentences):\n",
    "            for e in s[0]:\n",
    "                lambda_total[e] = 0 # fout\n",
    "                for f in s[1]:\n",
    "                    expectation = count_e2f[e][f] * theta[e][f] / theta_total[k][f]\n",
    "                    lambd[e][f] = alpha + expectation\n",
    "                    lambda_total[e] += lambd[e][f]\n",
    "        end = time.time()\n",
    "        print('third loop: {}'.format(end-start))\n",
    "        \n",
    "        # Store some results for plot\n",
    "        house_maison.append(lambd['house']['maison'])\n",
    "        house_demain.append(lambd['house']['demain'])\n",
    "                    \n",
    "        \n",
    "        # Simulation of a close-to Viterbi algorithm to calculate the AER for each iteration\n",
    "        alignments = []\n",
    "        for s in val_sentences:\n",
    "            alignments.append(align_IBM1(s,lambd))\n",
    "        als.append(alignments)\n",
    "                    \n",
    "    return lambd, perplexity_vals, als, house_maison, house_demain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first loop: 79.24879002571106\n",
      "Iteration 0\n",
      "second loop: 546.9475557804108\n",
      "third loop: 181.96502995491028\n"
     ]
    }
   ],
   "source": [
    "lambd_bayes, perplexity_vals_bayes, als_bayes, hm_bayes, hd_bayes = VIB(sentences_u, validation_sentences_u, 0.001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_naacl_IBM1(t, test_sentences, path):\n",
    "    f = open(path, \"w+\")\n",
    "    N = len(test_sentences)\n",
    "    for n, s in enumerate(test_sentences):\n",
    "        l = len(s[0])\n",
    "        m = len(s[1])\n",
    "        for i in range(l):\n",
    "            if i==0: continue\n",
    "            vals=[]\n",
    "            for j in range(m):\n",
    "                vals.append(t[s[0][i]][s[1][j]])\n",
    "            max_idx = vals.index(max(vals))\n",
    "            if n==N-1 and i==l-1 and j==m-1:\n",
    "                f.write(\"{} {} {} S\".format(n+1, i, max_idx+1))\n",
    "            else:\n",
    "                f.write(\"{} {} {} S\\n\".format(n+1, i, max_idx+1))\n",
    "    f.close()\n",
    "    \n",
    "create_naacl_IBM1(lambd_bayes, validation_sentences_u, \"ibm1.vb.naacl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
